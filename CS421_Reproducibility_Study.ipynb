{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Reproducibility Study for SusGen-GPT**\n",
        "\n",
        "In this notebook we setup and fine-tune a `Mistral 7B` model in a very similar way to how the author's of the **SusGen-GPT** paper do! We leverage Google Colab Pro's free monthly compute allowance of 100 CUs a month and their T4 GPU to work in a low compute environment (but one that at least has access to a GPU). Given the low resources, this training is sensitive in regards to memory and crashes or Out Of Memory errors. Also, Google Colab does not seem to gel well with `bitsandbytes` or with `QLoRA` implementations, as to get it to work you have to be on a earlier Python version (Python 3.10). However, Colab does not easily let you use a version that is that old, so lots of work arounds are needed and it is very finicky. Nevertheless, this notebook describes through markdown cells the process to reproduce this work in a low compute environment using the author's code and implementation strategies!"
      ],
      "metadata": {
        "id": "LmxgiJhARG9H"
      },
      "id": "LmxgiJhARG9H"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we mount our Google Drive so we can access the author's source code that we uploaded and use Colab's T4 GPU."
      ],
      "metadata": {
        "id": "kWERq-rfK2Hi"
      },
      "id": "kWERq-rfK2Hi"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46COsAgHXrGq"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "46COsAgHXrGq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Navigate to the SusGen repository, wherever that is stored in your Google Drive!"
      ],
      "metadata": {
        "id": "9LekAmAWLMx3"
      },
      "id": "9LekAmAWLMx3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvunFN08XrGu"
      },
      "source": [
        "# Navigate to SusGen repo (path will change depending on your setup!)\n",
        "%cd /content/drive/My Drive/cs421/Reproducibility_Study/SusGen"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "mvunFN08XrGu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we install dependencies and setup the environment using the `requirements.txt` file the author's put together."
      ],
      "metadata": {
        "id": "5jhxgUoWLT-B"
      },
      "id": "5jhxgUoWLT-B"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDLF02YWXrGu"
      },
      "source": [
        "# Install dependencies\n",
        "%env VLLM_INSTALL_PUNICA_KERNELS=1\n",
        "!pip install -r requirements.txt"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "xDLF02YWXrGu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we load the  `SusGen-10K` dataset.\n",
        "I added the 10K dataset to the SusGen folder in Google Drive for convenience, but the dataset is publically available and can be downloaded from the below link and loaded from wherever it is convenient. I also added the link to the `SusGen-30K` dataset in case you don't have the limitations I have and can use the larger dataset.\n",
        "\n",
        "- Link to `SusGen-10K` dataset: https://huggingface.co/datasets/WHATX/SusGen-30k/blob/main/ablation/SusGen-10k.json\n",
        "\n",
        "- Link to `SusGen-30K` dataset: https://huggingface.co/datasets/WHATX/SusGen-30k"
      ],
      "metadata": {
        "id": "OMLsOCsvLi50"
      },
      "id": "OMLsOCsvLi50"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9SHhHkAXrGv"
      },
      "source": [
        "# Load SusGen-10K dataset\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('json', data_files='/content/drive/My Drive/cs421/Reproducibility_Study/SusGen/SusGen-10k.json')\n",
        "dataset"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "_9SHhHkAXrGv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we format the dataset to ensure it is in an instruction-response format, which is expected for this training and fine-tuning that is detailed by the authors in the paper. We make sure the dataset is composed of two parts, the `prompt` and `response`. The `prompt` is the instruction + input, and the `response` is the output."
      ],
      "metadata": {
        "id": "dZDz0eF_MvLV"
      },
      "id": "dZDz0eF_MvLV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDsXH39kXrGv"
      },
      "source": [
        "def format_example(example):\n",
        "  instruction = example['instruction']\n",
        "  inp = example.get('input', '')\n",
        "  # response is the output\n",
        "  output = example['output']\n",
        "  if inp and len(inp.strip()) > 0:\n",
        "    # prompt is instruction + input\n",
        "    prompt = f\"<s>[INST] {instruction}\\n{inp} [/INST]\"\n",
        "  else:\n",
        "    prompt = f\"<s>[INST] {instruction} [/INST]\"\n",
        "\n",
        "  return {'prompt':prompt, 'response':output}\n",
        "\n",
        "# Apply this format to the entire dataset to ensure it is the proper format\n",
        "processed = dataset.map(format_example)\n",
        "processed"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "UDsXH39kXrGv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we load the `Mistral 7B` model in quantized form, which helps alleviate the memory pressure of loading 7 billion parameters! We use the `BitsAndBytesConfig` to load the model in 4-bit quantized form, which really reduces the memory pressure, and we load the `tokenizer` as well."
      ],
      "metadata": {
        "id": "K8qKvRD6NkRO"
      },
      "id": "K8qKvRD6NkRO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ_JjB5EXrGv"
      },
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-v0.3\"\n",
        "# Configure bitsandbytes to use for loading the model in quantized form\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "# Load tokenizer for the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "# Load the Mistral 7B model in quantized form\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "eJ_JjB5EXrGv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we apply `QLoRA` to the `Mistral 7B` model we loaded in above. This allows us to fine tune only Low Rank Adapters while keeping the base model weights frozen. We define the `LoraConfig` in the same way the authors did originally to keep in step with their setup."
      ],
      "metadata": {
        "id": "B2YBG6CqOd7m"
      },
      "id": "B2YBG6CqOd7m"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJt-Nze3XrGv"
      },
      "source": [
        "# Apply QLoRA\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "# Prepare loaded model for quantized training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "# Setup the lora config following the authors implementation\n",
        "lora_config = LoraConfig(r=16, lora_alpha=32, lora_dropout=0.1, task_type='CAUSAL_LM')\n",
        "# Add the trainable LoRA parameters to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "XJt-Nze3XrGv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we tokenize the dataset so we can pass it to the LLM for training. We use our `tokenizer` to convert the text as well as the labels into tokens, ensuring padding and truncation are handled, and we set a max length for each prompt and label."
      ],
      "metadata": {
        "id": "aatxOX-zPIeQ"
      },
      "id": "aatxOX-zPIeQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_Z1VVvsXrGw"
      },
      "source": [
        "# Tokenize dataset\n",
        "def tokenize(batch):\n",
        "  # Tokenize the prompts\n",
        "  tok = tokenizer(batch['prompt'], padding='max_length', truncation=True, max_length=1024)\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    # Tokenize the labels\n",
        "    labels = tokenizer(batch['response'], padding='max_length', truncation=True, max_length=512)\n",
        "    tok['labels'] = labels['input_ids']\n",
        "    return tok\n",
        "# Apply tokenization to the whole dataset we processed earlier\n",
        "tokenized = processed['train'].map(tokenize, batched=True, remove_columns=processed['train'].column_names)\n",
        "tokenized"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "j_Z1VVvsXrGw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we set up the training for our model with all the hyperparameter settings we chose (see report for details on choices). We use `Trainer` from the `transformers` library to simplify the pipeline for training, and in the next cell we begin the training by calling `trainer.train()`!"
      ],
      "metadata": {
        "id": "22EM1BAjQGV5"
      },
      "id": "22EM1BAjQGV5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDUDFTHVXrGw"
      },
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "# Set the hyperparameters and arguments for training\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/My Drive/cs421/Reproducibility_Study/SusGen/outputs/mistral7b_lora\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=1e-5,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "# Define the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized,\n",
        "    data_collator=collator,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "mDUDFTHVXrGw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcuih8ERXrGw"
      },
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "Zcuih8ERXrGw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we save the trained model (paths will be different depending on your Drive setup).  "
      ],
      "metadata": {
        "id": "n4_QuH-HQwhH"
      },
      "id": "n4_QuH-HQwhH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNS57pGsXrGx"
      },
      "source": [
        "# Save LoRA adapter\n",
        "trainer.model.save_pretrained('/content/drive/My Drive/cs421/Reproducibility_Study/SusGen/outputs/mistral7b_lora_adapter')\n",
        "tokenizer.save_pretrained('/content/drive/My Drive/cs421/Reproducibility_Study/SusGen/outputs/mistral7b_lora_adapter')"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "PNS57pGsXrGx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we load the trained model and run the evaluation code the authors wrote to get the results of our fine-tuned model on their benchmarks."
      ],
      "metadata": {
        "id": "-vQ36xgaQ8K8"
      },
      "id": "-vQ36xgaQ8K8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE3E-kEqXrGx"
      },
      "source": [
        "# Load for evaluation\n",
        "from peft import PeftModel\n",
        "base = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map='auto')\n",
        "lora_model = PeftModel.from_pretrained(base, '/content/drive/My Drive/cs421/Reproducibility_Study/SusGen/outputs/mistral7b_lora_adapter')"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "RE3E-kEqXrGx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buStCo8QXrGx"
      },
      "source": [
        "# Run authors' evaluation script\n",
        "!python eval/code/eval.py --model-path /content/drive/MyDrive/SusGen/outputs/mistral7b_lora_adapter"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "buStCo8QXrGx"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}